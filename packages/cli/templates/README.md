# Glubean API Tests

This project was generated by `glubean init`.

## Project Structure

```
├── tests/                  # Permanent tests — run in CI, Cloud, and locally
├── explore/                # Exploratory tests — quick iteration in your editor
├── data/                   # Shared test data (JSON, CSV, YAML) used by both
├── context/                # API specs and reference docs for AI and tooling
├── deno.json               # Config: tasks, SDK import, testDir/exploreDir
├── .env                    # Default environment variables (BASE_URL, etc.)
├── .env.secrets            # Default secrets (API keys — gitignored)
├── .env.staging            # Staging environment variables
├── .env.staging.secrets    # Staging secrets (gitignored)
└── AGENTS.md               # AI agent guidelines
```

**Required:** `tests/` and `deno.json`. Everything else is recommended.

- **`tests/`** — Your permanent test files (`*.test.ts`). These are what `deno task test` and CI run. All test files must end in `.test.ts`.
- **`explore/`** — Same `*.test.ts` files, but for interactive exploration. Think of it as a scratchpad: quick API calls, debugging, prototyping. Move files to `tests/` when they're ready to be permanent. Run with `deno task explore`.
- **`data/`** — Shared test data files (JSON, CSV, YAML). Both `tests/` and `explore/` can import from here. Keeps test logic clean and data reviewable.
- **`context/`** — API specs (OpenAPI), reference docs, and anything that helps you (or your AI) understand the API under test. The `glubean context` command reads from here to generate AI-optimized context.

The `data/` and `context/` directories are optional conventions — you can organize data and specs however you prefer. These defaults exist because they work well with the CLI tooling and AI workflows.

## Prerequisites

1. Install [Deno](https://deno.land) (v2+):

```bash
# macOS
brew install deno

# or see https://docs.deno.com/runtime/getting_started/installation/
```

2. Install the Glubean CLI:

```bash
deno install -Ag -n glubean jsr:@glubean/cli
```

## Quick start

1. Edit `.env` with your API base URL
2. Edit `.env.secrets` with any API keys
3. Run tests:

```bash
deno task test            # run all tests in tests/
deno task test:verbose    # with detailed output
deno task test:staging    # run against staging environment
deno task explore         # run explore/ tests
```

Or run a specific file:

```bash
glubean run tests/demo.test.ts --verbose
```

## Environment switching

The same tests can run against different environments by switching env files.
Each env file pairs with a secrets file automatically:

| Env file       | Secrets file           | Task shortcut            |
| -------------- | ---------------------- | ------------------------ |
| `.env`         | `.env.secrets`         | `deno task test`         |
| `.env.staging` | `.env.staging.secrets` | `deno task test:staging` |

To add more environments (e.g. production), create `.env.production` and `.env.production.secrets`, then run:

```bash
glubean run --env-file .env.production
```

Or add a task in `deno.json`:

```json
"test:production": "deno run -A jsr:@glubean/cli run --env-file .env.production"
```

All `*.secrets` files are gitignored by default. The non-secret `.env.*` files can be committed safely — they typically only contain `BASE_URL` and other non-sensitive config.

## AI-assisted development tools

Glubean includes CLI tools that help AI agents write better tests:

### `glubean context` — Generate AI context

```bash
# Generate .glubean/ai-context.md with SDK reference, test patterns, and coverage gaps
glubean context --openapi context/openapi.sample.json
```

The generated file is referenced in `AGENTS.md` and can be used with `@.glubean/ai-context.md` in Cursor.

### `glubean diff` — Show OpenAPI changes

```bash
# Show what changed vs last commit
glubean diff

# Compare against a specific ref
glubean diff --base main
```

### `glubean coverage` — Show endpoint test coverage

```bash
# Show which endpoints have tests (requires running tests first)
deno task test
glubean coverage --openapi context/openapi.sample.json
```

---

## Use your AI to generate tests (from OpenAPI)

This project includes a sample OpenAPI spec at `context/openapi.sample.json`.
Most AI coding tools can use this spec to generate accurate tests quickly.

Copy/paste prompts you can try (edit as needed):

1. Generate a smoke suite:

> Read `context/openapi.sample.json` and `AGENTS.md`. Generate Glubean tests in `tests/smoke.test.ts` that cover all GET endpoints with 200 responses. Use the builder API with `.step()` for multi-step flows. Use `ctx.http` for requests (auto-traces every call). Add assertions for status code and basic response schema. Group tests by tag.

2. Generate auth + negative tests:

> Using `context/openapi.sample.json`, generate tests in `tests/auth.test.ts` that verify authentication behavior: missing API key returns 401, invalid API key returns 401, insufficient scope returns 403. Use the builder API with `.setup()` for shared auth headers. Use `ctx.secrets.require("API_KEY")` and also test missing/empty API key scenarios safely.

3. Generate CRUD flow tests:

> From `context/openapi.sample.json`, identify resources with create/read/update/delete endpoints. Generate builder API tests in `tests/crud.test.ts` with steps: create → verify → update → verify → delete → verify. Pass state (IDs) between steps.

4. Generate validation tests:

> From `context/openapi.sample.json`, identify POST/PUT endpoints with request bodies. Generate tests in `tests/validation.test.ts` for validation edge cases (missing required fields, invalid enums, min/max constraints). Assert 422 responses and validate the error schema shape.

## Try MCP in Cursor (AI closed loop)

Glubean also ships an MCP (Model Context Protocol) server so your AI tool can **discover tests**, **run them**, read **structured failures**, and iterate quickly:

> AI writes tests → runs locally → reads assertions/logs/traces → fixes → reruns → ✓ pass

### Setup

1. Configure the MCP server in Cursor (via UI or `~/.cursor/mcp.json`):

```json
{
  "mcpServers": {
    "glubean": {
      "command": "deno",
      "args": ["run", "-A", "jsr:@glubean/mcp"],
      "description": "Glubean test runner and cloud integration"
    }
  }
}
```

2. Restart Cursor.

### Prompts to try

- Generate + run + fix (local):

> Use the Glubean MCP tools to discover tests in `tests/`, run them locally, and if any fail, fix the test code and rerun until everything passes. Keep changes minimal and explain each fix.

- Generate tests from spec + run (local):

> Read `context/openapi.sample.json` and generate Glubean tests in `tests/smoke.test.ts`. Then use MCP to run them locally. If validation or auth failures happen, adjust payloads/headers and rerun until green.

### Optional: enable cloud tools

Local MCP tools do not require cloud auth. If you also want the AI to trigger remote runs via the Open Platform, set:

```bash
export GLUBEAN_TOKEN=glb_token_xxx
export GLUBEAN_API_URL=https://api.glubean.com
```

## What you get from the cloud (and why it matters)

Running locally is great for authoring and debugging. The cloud is where the
same verification code becomes a **continuous operational workflow** — no
rewrites, no extra config files, no separate CI pipeline.

### The problem Glubean Cloud solves

| Without Glubean Cloud                            | With Glubean Cloud                                                      |
| ------------------------------------------------ | ----------------------------------------------------------------------- |
| Tests run in CI, pass, and are forgotten         | Tests run on a schedule — every 5 min, hourly, daily                    |
| Staging tested, production assumed OK            | Same tests, different environments, zero code changes                   |
| API breaks at 3am, discovered next morning       | Slack/email alert within minutes with exact failure details             |
| Test results are ephemeral terminal output       | Structured history: assertions, traces, logs — searchable and shareable |
| Secrets hardcoded or scattered across CI configs | Encrypted environment groups, injected at runtime, never in git         |

### What the cloud provides

- **Automatic bundle builds**: `git push` triggers a build — your test files are
  packaged into an immutable bundle (like a Docker image for verification).
- **Environment groups**: configure `staging` and `production` with different
  `BASE_URL`, API keys, and secrets. Run the same tests against both.
- **Scheduling**: every N minutes, daily at a specific time, weekly, or cron.
  Workers pick up runs from a queue — nothing to maintain.
- **Alerts**: Slack, email, or webhooks. Failures include the exact assertion
  that broke (`expected 200, got 502`), not a generic "tests failed" message.
- **Dashboard**: full run history with assertion timelines, API traces, HTTP
  metrics, and step-level breakdowns.
- **Open Platform**: project-scoped tokens and webhooks for integrating results
  into your internal tools and automation.

### Get started with the cloud

1. Create an account at `https://app.glubean.com`
2. Create a Project and connect your GitHub repository
3. Push your tests:

```bash
git add .
git commit -m "add api tests"
git push
```

4. Glubean builds your bundle automatically. Click **Run Now** in the dashboard,
   or set up a schedule to run continuously.

## Notes

- `.env.secrets` should stay local and must not be committed.
- Edit test files in `tests/` to match your real API behavior and contracts.
